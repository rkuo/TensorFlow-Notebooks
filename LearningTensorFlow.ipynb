{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning TensorFlow\n",
    "![tensorflow logo][1]\n",
    "\n",
    "[1]: https://upload.wikimedia.org/wikipedia/en/7/74/TensorFlow.png"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is study note of reading the [whitepaper-TensorFlow:Large-Scale Machine Learning on Heterogeneous Distributed Systems][5] Some notes are shamelessly copied from the whitepaper.\n",
    "\n",
    "## About TensorFlow\n",
    "\n",
    "***TensorFlow is an interface for expressing machine learning algorithms, and an implementation for executing such algorithms.***\n",
    "\n",
    "* A Framework that allows a developer to express his machine learning algorithm symbolically, performing compilation of these statements and executing them.\n",
    "* A programming metaphor that requires the developer to model the machine learning algorithm as a computation graph.\n",
    "    * Nodes in the graph represent mathematical operations  \n",
    "    * Edges in the graph represent the multidimensional data arrays (called tensors), which is used to communicate between nodes.\n",
    "\n",
    "* A set of Python classes and methods that provide an API interface \n",
    "* A re-targetable system that can run on different hardware\n",
    "\n",
    "TensorFlow supports the computation on one or more CPUs or GPUs in a desktop, server, or mobile device with a single API. It can be deployed to a cloud ([Google Cloud Platform][4], [AWS][3], ...) or to a container. \n",
    "\n",
    "Google have open-sourced the TensorFlow API and a reference implementation under the Apache 2.0 license in November, 2015, available at [www.tensorflow.org][2].\n",
    "\n",
    "[2]: https://www.tensorflow.org/\n",
    "[3]: https://aws.amazon.com/marketplace/pp/B01AOE205O\n",
    "[4]: https://cloud.google.com/ml/\n",
    "[5]: http://download.tensorflow.org/paper/whitepaper2015.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Programming Model and Basic Concepts\n",
    "\n",
    "A TensorFlow computation is described by a [directed graph][7], which is composed of a set of nodes. The graph represents a dataflow computation, with extensions for allowing some kinds of nodes to maintain and update\n",
    "persistent state and for branching and looping control structures within the graph in a manner similar to [Naiad][6]. \n",
    "\n",
    "![TensorFlow][5] \n",
    "\n",
    "[5]: https://www.tensorflow.org/images/tensors_flowing.gif\n",
    "[6]: http://research.microsoft.com:8082/pubs/201100/naiad_sosp2013.pdf\n",
    "[7]: https://en.wikipedia.org/wiki/Directed_acyclic_graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clients typically construct a computational graph using one of the supported frontend languages (C++ or Python). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Operations and Kernels\n",
    "\n",
    "An operation has a name and represents an abstract computation (e.g., “matrix multiply”, or “add”). An operation can have attributes, and all attributes must be provided or inferred at graph-construction time in order to instantiate a node to perform the operation. \n",
    "\n",
    "A kernel is a particular implementation of an operation that can be run on a particular type of device (e.g., CPU or GPU).\n",
    "\n",
    "Table 1 shows some of the kinds of operations built into the core\n",
    "TensorFlow library."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Operation Types\n",
    "\n",
    "Table-1\n",
    "\n",
    "|Operation                            |Examples                                                       |\n",
    "|-------------------------------------|-------------------------------------------------------|\n",
    "|Element-wise mathematical operations |Add, Sub, Mul, Div, Exp, Log, Greater, Less, Equal, ...|\n",
    "|Array operations |Concat, Slice, Split, Constant, Rank, Shape, Shuffle, ...|\n",
    "|Matrix operations | MatMul, MatrixInverse, MatrixDeterminant, ...|\n",
    "|Stateful operations| Variable, Assign, AssignAdd, ...|\n",
    "|Neural-net building blocks |SoftMax, Sigmoid, ReLU, Convolution2D, MaxPool, ...|\n",
    "|Checkpointing operations |Save, Restore |\n",
    "|Queue and synchronization operations |Enqueue, Dequeue, MutexAcquire, MutexRelease, ...|\n",
    "|Control flow operations | Merge, Switch, Enter, Leave, NextIteration|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Session\n",
    "Clients programs interact with the TensorFlow system by creating a `Session`. The initial graph when a session is created is empty.\n",
    "\n",
    "The Session interface supports an `Extend` method to augment the current graph managed by the session with additional nodes and edges.\n",
    "\n",
    "The main operation is `Run`, which \n",
    "- takes a set of output names that need to be computed, as well as an optional set of tensors to be fed into the graph in place of certain outputs of nodes. \n",
    "- Using the arguments to Run, the TensorFlow implementation can compute the transitive closure of all nodes that must be executed in order to compute the outputs that were requested, and can then arrange to execute the appropriate nodes in an order that respects their dependencies. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Variable \n",
    "is a special kind of **operation** that returns a handle to a persistent mutable tensor that survives across executions of a graph. Handles to these persistent mutable tensors can be passed to a handful of special operations, such as `Assign` and `AssignAdd` (equivalent to +=) that mutate the referenced tensor. \n",
    "\n",
    "For machine learning applications of TensorFlow, the parameters of the model are typically stored in tensors held in variables, and are updated as part of the Run of the training graph for the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Work Flow\n",
    "\n",
    "Build a Graph -> Initialize Sessions -> Run Sessions\n",
    "1. Perform any pre-processing steps to set up the dataset for the classifier.\n",
    "2. Set up the variables (such as the Weights and biases) and placeholders (such as the input vector) that are fed at runtime\n",
    "3. Define the required computations. E.g. you may define the dot_product computation using the matmul method of tensorflow feeding it the weight matrix variable and the input vector (that may be defined as a placeholder).\n",
    "\n",
    "        Till this point nothing is executed by Tensorflow\n",
    "\n",
    "4. Start a session instance and initialize all variables\n",
    "5. Run the session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Example-1\n",
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "c1 = tf.constant([1,2,3,4])\n",
    "c2 = tf.constant([-1,2,-3,4])\n",
    "y = c1 + c2 # symbolic add"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "y is only a place holder, nothing happen at this point. If we `run` it, we will get value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"add_1:0\", shape=(4,), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "print(y)  # this gives us type information. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# create a session\n",
    "sess = tf.Session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y = sess.run(y)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 4 0 8]\n"
     ]
    }
   ],
   "source": [
    "print(y)  # we can get value now"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation\n",
    "\n",
    "TensorFlow has two main components: \n",
    "\n",
    "* client, which uses the Session interface to communicate with the master, \n",
    "* and one or more worker processes, with each worker process responsible for arbitrating access to one or more computational devices (such as CPU cores or GPU cards) and for executing graph nodes on those devices as instructed by the master. \n",
    "\n",
    "Tensorflow can be implemented either local or distributed.\n",
    "\n",
    "* Local: the client, the master, and the worker all run on a single machine\n",
    "* Distributed: the client, the master, and the workers can all be in different processes on different machines\n",
    "\n",
    "![tensorflow execution modes][1]\n",
    "\n",
    "#### Devices\n",
    "\n",
    "Each worker is responsible for one or more devices, and each device has a device type, and a name. Device\n",
    "names are composed of pieces that identify the device’s type, the device’s index within the worker, and,\n",
    "in our distributed setting, an identification of the job and task of the worker (or localhost for the case where\n",
    "the devices are local to the process). \n",
    "\n",
    "name = type + distributed setting + index   \n",
    "type = [cpu | gpu]   \n",
    "index = 0, 1, ...   \n",
    "distributed setting = task:number   \n",
    "\n",
    "Example device names are \"/job:localhost/device:cpu:0\" or \"/job:worker/task:17/device:gpu:3\". We have implementations of our Device interface for CPUs and GPUs, and new device implementations for other device types can be provided via a registration mechanism. Each device object is responsible for managing allocation and deallocation of device memory, and for arranging for the execution of any kernels that are requested by higher levels in the TensorFlow implementation.\n",
    "\n",
    "#### Tensors\n",
    "\n",
    "A tensor is a typed, multidimensional array. \n",
    "\n",
    "Supported types including \n",
    "* signed and unsigned integers ranging in size from 8 bits to 64 bits,   \n",
    "* IEEE float and double types,   \n",
    "* a complex number type, and    \n",
    "* a string type (an arbitrary byte array).   \n",
    "\n",
    "[1]: http://thenewstack.io/wp-content/uploads/2015/11/tensorflow-3.png"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Single Device Execution\n",
    "\n",
    "The nodes of the graph are executed in an order that respects the dependencies between nodes. A node will track its dependencies, whether they have been executed or not. Once this dependency count drops to zero, the node is eligible for execution and is added to a ready-queue. The ready-queue is processed in some unspecified order, delegating execution of the kernel for a node to the device object. When a node has finished executing, the counts of all nodes that depend on the completed node are decremented.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multiple Devices Execution\n",
    "\n",
    "When a system has multiple devices, there are two more concerns: \n",
    "- deciding which device to place the computation for each node in the graph, and then \n",
    "- managing the required communication of data across device boundaries implied by these placement decisions.\n",
    "\n",
    "#### Node Placement\n",
    "For a given computation graph, one of the main responsibilities of the TensorFlow implementation is to map the computation onto the set of available devices. \n",
    "\n",
    "One input to the placement algorithm is a cost model, which contains \n",
    "- estimates of the sizes (in bytes) of the input and \n",
    "- output tensors for each graph node, along with \n",
    "- estimates of the computation time required for each node when presented with its input tensors. \n",
    "\n",
    "This cost model is either statically estimated based on \n",
    "- heuristics associated with different operation types, or is \n",
    "- measured based on an actual set of placement decisions for earlier executions of the graph.\n",
    "\n",
    "The placement algorithm first runs a simulated execution of the graph. The simulation is described below and ends up picking a device for each node in the graph using greedy heuristics. The node to device placement generated by this simulation is also used as the placement for\n",
    "the real execution.\n",
    "\n",
    "The placement algorithm starts with the sources of the computation graph, and simulates the activity on each device in the system as it progresses. For each node that is reached in this traversal, the set of feasible devices is considered (a device may not be feasible if the device does not provide a kernel that implements the particular operation). \n",
    "\n",
    "For nodes with multiple feasible devices, the placement algorithm uses a greedy heuristic that examines the effects on the completion time of the node of placing the node on each possible device. This heuristic takes into account the estimated or measured execution time of the operation on that kind of device from the cost model, and also includes the costs of any communication that would be introduced in order to transmit inputs\n",
    "to this node from other devices to the considered device. The device where the node’s operation would finish the soonest is selected as the device for that operation, and the placement process then continues onwards to make placement decisions for other nodes in the graph, including\n",
    "downstream nodes that are now ready for their own simulated execution. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distributed Execution \n",
    "\n",
    "Send/Receive node pairs that communicate\n",
    "across worker processes use remote communication\n",
    "mechanisms such as TCP or RDMA to move data\n",
    "across machine boundaries.\n",
    "\n",
    "Fault Tolerance\n",
    "Failures in a distributed execution can be detected in a\n",
    "variety of places. The main ones we rely on are (a) an\n",
    "error in a communication between a Send and Receive\n",
    "node pair, and (b) periodic health-checks from the master\n",
    "process to every worker process.\n",
    "When a failure is detected, the entire graph execution\n",
    "is aborted and restarted from scratch. Recall however\n",
    "that Variable nodes refer to tensors that persist across executions\n",
    "of the graph. We support consistent checkpointing\n",
    "and recovery of this state on a restart. In partcular,\n",
    "each Variable node is connected to a Save node. These\n",
    "Save nodes are executed periodically, say once every N\n",
    "iterations, or once every N seconds. When they execute,\n",
    "the contents of the variables are written to persistent storage,\n",
    "e.g., a distributed file system. Similarly each Variable\n",
    "is connected to a Restore node that is only enabled\n",
    "in the first iteration after a restart. See Section 4.2 for\n",
    "details on how some nodes can only be enabled on some\n",
    "executions of the graph.\n",
    "\n",
    "![inter-node communication][1]\n",
    "\n",
    "[1]:https://1.bp.blogspot.com/-aaFMXb42B-k/V1D6ckGAhjI/AAAAAAAAFtU/5A6r2kGeA5sg0k2PNXxm2QcUpjvEKQxzgCKgB/s320/Screen%2BShot%2B2016-06-02%2Bat%2B10.42.04%2BPM.png"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TensorBoard\n",
    "\n",
    "![tensorboard diagram][1]\n",
    "[1]: https://www.tensorflow.org/versions/r0.9/images/graph_vis_animation.gif"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reference\n",
    "[Whitepaper][1]   \n",
    "[AI and Deep Machine Learning Primer][2]    \n",
    "\n",
    "[1]: http://download.tensorflow.org/paper/whitepaper2015.pdf\n",
    "[2]: http://a16z.com/2016/06/10/ai-deep-learning-machines/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
